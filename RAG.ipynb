{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub[pandas-datasets] langchain faiss-cpu sentence-transformers transformers\n",
    "pip install -U langchain-community\n",
    "pip install -U sentence-transformers transformers torch sympy\n",
    "pip install kagglehub\n",
    "# Wiki data\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "# Set the path\n",
    "file_path = \"test.csv\"\n",
    "df = kagglehub.load_dataset(\n",
    "KaggleDatasetAdapter.PANDAS,\n",
    "\"thedevastator/wikipedia-biographies-text-generation-dataset\",\n",
    "file_path,)\n",
    "df50 = df.head(50)\n",
    "# Convert to document langchain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=row[\"input_text\"], metadata={\"name\": row.get(\"name\", str(i))})\n",
    "    for i, row in df50.iterrows()\n",
    "]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "# Victor db with embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)                                        # TAKES A LOT OF TIME\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "# Load llm from hugging face (local)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Or any instruct model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# Prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a security assistant trained to create creative passwords.\n",
    "You will be given a short person description. First, you will receive a few similar profiles.\n",
    "Then, you must create a strong password that combines:\n",
    "- important traits from the original description\n",
    "- references to similar people\n",
    "- use a mix of symbols, digits, and capitalization\n",
    "\n",
    "Original description:\n",
    "\"{query}\"\n",
    "\n",
    "Similar people:\n",
    "{context}\n",
    "\n",
    "Now generate a password that encodes the theme.\n",
    "Only return the password, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"query\", \"context\"], template=template)\n",
    "rag_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "# Password generation func\n",
    "def generate_contextual_password(description: str) -> str:\n",
    "    similar_docs = retriever.get_relevant_documents(description)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in similar_docs)\n",
    "    return rag_chain.run({\"query\": description, \"context\": context})\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    desc = \"A female british works in computer science\"\n",
    "    password = generate_contextual_password(desc)\n",
    "    print(\"Generated Password:\", password)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
